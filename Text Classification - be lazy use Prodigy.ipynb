{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Text Classification: Be lazy, use Prodigy !\n",
    "\n",
    "Text Classification could be complex to tune and to implement. If your objective is satisfied with a good-enough, ready-to-production, easy-to-upgrade model, use [Prodigy](https://prodi.gy/) to train a smooth classifier for [spaCy](https://spacy.io/), the production-ready python framework for NLP.\n",
    "\n",
    "Text classification is often a great exercice to deep dive into NLP technics, because you can test and apply a lot of tools: from TF-IDF to words embeddings, training your own doc2vec/word2vec, applying some classic classifiers, testing neural net like RNN, etc.\n",
    "\n",
    "Indeed, you can spent **HUGE** time to build-up your classification strategy and to improve your algorithms: you will augment your data, create new features, try new tricks to increase the accuracy of your models, etc. It's endless. If you're lucky, you will find the gold feature to kick-up your model on your specific data, and if you're even more lucky, a secret sauce to generalize it on a wider domain.\n",
    "But it's hard. Sometime, it's really specific to your business domain (it could depend on the used vocabulary, the size of sentences, etc.) and it usually takes time to be tuned… that could be not compatible with a production environment where you have to quickly provide several classifiers and maintain them in time.\n",
    "Then, If your objective will be satisfied with a good-enough (i.e. not 0.9999), ready-to-production, easy-to-upgrade model, I suggest to test and use Prodigy to train a classification model usable with spaCy, the production-ready python framework for NLP. \n",
    "\n",
    "# Prodigy\n",
    "As explained by its creators, [Prodigy](https://prodi.gy/) is : \n",
    "\n",
    "`\n",
    "an annotation tool so efficient that data scientists can do the annotation themselves, enabling a new level of rapid iteration. Whether you’re working on entity recognition, intent detection or image classification, Prodigy can help you train and evaluate your models faster. Stream in your own examples or real-world data from live APIs, update your model in real-time and chain models together to build more complex systems.\n",
    "`\n",
    "\n",
    "So Prodigy will provide you web interfaces to tag you data (texts, images, etc.) but also command lines to train new models. That's this last part I will use in my example to train a text classifier.\n",
    "\n",
    "## Example Description : Spooky Competition\n",
    "For my example, I am using the dataset provided by the [Spooky Kaggle Competition](), where the objective was to predict the author of excerpts from horror stories by Edgar Allan Poe (EAP), Mary Shelley (MWS), and HP Lovecraft (HPL).\n",
    "The tagging is already done, so I do not have to use the Prodigy's web interface for this time being (which is great, by the way!).\n",
    "\n",
    "What I have to do is:\n",
    "- To prepare the Data for Prodigy,\n",
    "- Build the dataset for prodigy,\n",
    "- and train a new model using the tool\n",
    "\n",
    "### Import libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, sys, time\n",
    "import json\n",
    "import csv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prepare Data for Prodigy\n",
    "Kaggle provides data in CSV. I create first some functions to manipulate json instead of csv files:\n",
    "- create_json : create a json file from a csv file\n",
    "- load_json : load json from a json file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_json(csv_file, json_file):\n",
    "    try :\n",
    "        if os.path.exists(csv_file) :\n",
    "            with open(csv_file) as f:\n",
    "                reader = csv.DictReader(f)\n",
    "                rows = list(reader)\n",
    "\n",
    "            with open(json_file, 'w') as f:\n",
    "                json.dump(rows, f)\n",
    "        else :\n",
    "            return {}\n",
    "    except :\n",
    "            return {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_json(json_file):\n",
    "    try :\n",
    "        if os.path.exists(json_file) :\n",
    "            with open(json_file) as f:\n",
    "                return json.load(f)\n",
    "        else :\n",
    "            return {}\n",
    "    except :\n",
    "            return {}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Build the dataset for Prodi.gy\n",
    "Build the data set for Prodi.gy:\n",
    "the function open a json, then retrieve:\n",
    "- **texts**: text of the sentece\n",
    "- **ids**: the id of the sentence\n",
    "- **labels**: the result (author) from train file. Retrieve nothing if missing (test file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_dataset(json_files):\n",
    "    ids = []\n",
    "    texts = []\n",
    "    labels = []\n",
    "\n",
    "    for jfile in json_files:\n",
    "        j = load_json(jfile)\n",
    "        for entry in j :\n",
    "            ids.append(entry['id'])\n",
    "            texts.append(entry['text'])\n",
    "            if entry.get('author'):\n",
    "                labels.append(entry['author'])\n",
    "    return texts, ids, labels"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It's time to create the input file for Prodigy.\n",
    "\n",
    "### Create the JSONL file for Prodigy\n",
    "\n",
    "Prodigy requires a jsonl file as input data, with a json dictionary per line.\n",
    "\n",
    "These json data must have the following parameters as minimal:\n",
    "`\n",
    "{“text”:”text of the sentence”,”label”:”category of the sentence”,”answer”:”’reject’ or ‘accept’”}\n",
    "`\n",
    "\n",
    "So, I have to create json data using:\n",
    "- the text of sentences and populating the text item,\n",
    "- the author of the sentences and populating the label items,\n",
    "- I also create a meta item, only for my convinience, using the id of the sentence,\n",
    "- and a answer item, describing the validity of the association between the text and the label.\n",
    "\n",
    "`\n",
    "Note regarding the answer item: prodigy request ‘reject’ and ‘accept’ data to train its model. If we only provide ‘accept’ data, the ouputs will not be good at all.\n",
    "So, I create ‘accept’ data with correct author, and ‘reject’ data with others.\n",
    "Example:\n",
    "The sentence “It never once occurred to me that the fumbling might be a mere mistake.” is from H.P. Lovecraft (HPL). So, the jsonl file must looks like:\n",
    "{“answer”: “reject”, “meta”: {“id”: “id17569”}, “text”: “It never once occurred to me that the fumbling might be a mere mistake.”, “label”: “MWS”}\n",
    "{“answer”: “accept”, “meta”: {“id”: “id17569”}, “text”: “It never once occurred to me that the fumbling might be a mere mistake.”, “label”: “HPL”}\n",
    "{“answer”: “reject”, “meta”: {“id”: “id17569”}, “text”: “It never once occurred to me that the fumbling might be a mere mistake.”, “label”: “EAP”}\n",
    "`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#path to folders\n",
    "path_to_json = './data/'\n",
    "save_dir = \"./save/\"\n",
    "\n",
    "#labels\n",
    "labels = ['MWS','HPL','EAP']\n",
    "\n",
    "#transform cvs in json files:\n",
    "create_json(path_to_json + \"train.csv\", path_to_json+ \"train.json\")\n",
    "create_json(path_to_json + \"test.csv\", path_to_json + \"test.json\")\n",
    "\n",
    "#Retrieve data from json files:\n",
    "json_file_train = [path_to_json + pos_json for pos_json in os.listdir(path_to_json) if pos_json == 'train.json']\n",
    "json_file_test = [path_to_json + pos_json for pos_json in os.listdir(path_to_json) if pos_json == 'test.json']\n",
    "\n",
    "texts_train, ids_train, labels_train = build_dataset(json_file_train)\n",
    "texts_test, ids_test, _ = build_dataset(json_file_test)\n",
    "\n",
    "#create jsonl file for prodigy:\n",
    "jsonl = open(save_dir + \"spooky.jsonl\", \"w\")\n",
    "for i in range(len(texts_train)):\n",
    "    line = {}\n",
    "    line['text'] = texts_train[i]\n",
    "    line['label'] = labels_train[i]\n",
    "    meta = {}\n",
    "    meta['id'] = ids_train[i]\n",
    "    line['meta'] = meta\n",
    "    for l in labels:\n",
    "        line['label'] = l\n",
    "        if l == labels_train[i]:\n",
    "            line['answer'] ='accept'\n",
    "        else:\n",
    "            line['answer'] ='reject'            \n",
    "        jsonl.writelines(json.dumps(line) + \"\\n\")\n",
    "jsonl.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Now you have to train the model using Prodigy\n",
    "\n",
    "I first create the dataset for my task:\n",
    "\n",
    "`\n",
    "bash-3.2$ prodigy dataset spooky\n",
    "\n",
    "`\n",
    "Thsn upload your jsonl:\n",
    "`\n",
    "\n",
    "bash-3.2$ prodigy db-in spooky path_to_your_jsonl\n",
    "`\n",
    "\n",
    "And train the model:\n",
    "\n",
    "`\n",
    "bash-3.2$ prodigy textcat.batch-train spooky --output spooky-model --eval-split 0.1 --n-iter 30\n",
    "`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import and test the model\n",
    "Now, I just have to import the created model into spaCy to use it. And it is really easy to do:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp = spacy.load('path_to_spooky-model')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "solution = [['id','EAP','HPL','MWS']]\n",
    "for i in range(len(texts_test)):\n",
    "    doc = nlp(texts_test[i])\n",
    "    solution.append([ids_test[i],doc.cats['EAP'],doc.cats['HPL'],doc.cats['MWS']])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create the result file\n",
    "write the results in a csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(save_dir + \"/result_pdy.csv\", \"w\") as f:\n",
    "    writer = csv.writer(f)\n",
    "    writer.writerows(solution)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
